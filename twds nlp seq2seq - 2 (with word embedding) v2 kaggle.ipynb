{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#import tensorflow as tf\n#print(tf.__version__)\n#print(tf.test.is_gpu_available())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nfrom keras.models import Model\nfrom keras.layers import Input, LSTM, Dense\n\nprint(\"DONE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/largetextfiles/world192small.txt', 'r', encoding='utf-8') as f:\n    text = f.readlines()\n\nwith open('../input/largetextfiles/world192small_baptista.txt', 'r', encoding='utf-8') as f:\n    text_encrypted = f.readlines()\n\nprint(\"DONE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(text))\nprint(len(text_encrypted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to store the samples\nen_samples = []\nde_samples = []\n\n# to store the characters\nen_chars = set()\nde_chars = set()\n\n# TODO.. cleanup.. maybe merge plain and encrypted beforehand\n# sanity check for next fori\nassert len(text) == len(text_encrypted)\n\n# split the samples and get the character sets\nfor i in range(len(text)):\n    en_ = text_encrypted[i]\n    de_ = text[i]\n    de_ = '\\t' + de_\n    for char in de_:\n        if char not in de_chars:\n            de_chars.add(char)\n    for char in en_:\n        if char not in en_chars:\n            en_chars.add(char)\n    en_samples.append(en_)\n    de_samples.append(de_)\n\nde_chars.add('\\n')\nde_chars.add('\\t')\n\nen_chars.add('\\n')\nen_chars.add('\\t')\n\nprint(\"DONE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# chars <-> ints convert\n\nde_char_to_int = dict()\nde_int_to_char = dict()\nen_char_to_int = dict()\nen_int_to_char = dict()\n\nfor i, char in enumerate(de_chars):\n    de_char_to_int[char] = i\n    de_int_to_char[i] = char\n    \nfor i, char in enumerate(en_chars):    \n    en_char_to_int[char] = i\n    en_int_to_char[i] = char\n\nprint(\"DONE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get lengths and sizes\nnum_en_chars = len(en_chars)\nnum_de_chars = len(de_chars)\n\nmax_en_chars_per_sample = max([len(sample) for sample in en_samples])\nmax_de_chars_per_sample = max([len(sample) for sample in de_samples])\n\nnum_en_samples = len(en_samples)\nnum_de_samples = len(de_samples)\n\nprint(\"DONE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initiate numpy arrays to hold the data for the seq2seq model\nencoder_input_data = np.zeros((num_en_samples,\n                               max_en_chars_per_sample,\n                               num_en_chars),\n                              dtype='float32')\n\ndecoder_input_data = np.zeros((num_de_samples,\n                               max_de_chars_per_sample,\n                               num_de_chars),\n                              dtype='float32')\n\ntarget_data = np.zeros((num_de_samples,\n                       max_de_chars_per_sample,\n                       num_de_chars),\n                      dtype='float32')\n\nprint(\"DONE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encode\n# takes too much space, thus 1/4 of original file\n# TODO.. next step: use embedding, words instead of chars\nfor i, (en_sample, de_sample) in enumerate(zip(en_samples, de_samples)):\n    for char, en_char in enumerate(en_sample):\n        encoder_input_data[i, char, en_char_to_int[en_char]] = 1\n    for char, de_char in enumerate(de_sample):\n        decoder_input_data[i, char, de_char_to_int[de_char]] = 1\n        if char > 0 :\n            target_data[i, char - 1, de_char_to_int[de_char]]  = 1\n            \nprint(\"DONE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #################################################################################################\n# #################################################################################################\n# #################################################################################################\n\n# END TWDS NLP1 P1\n\n# START TWDS NLP1 P2 / NLP2 ?\n\n# #################################################################################################\n# #################################################################################################\n# #################################################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DATA CLEANUP. OR IS IT?.. SHOULD RECONSIDER\n\nimport string, re\nfrom string import digits\n\n# Convert text to lowercase :\ntext_encrypted=list(map(lambda x: x.lower(), text_encrypted))\ntext=list(map(lambda x: x.lower(), text))\n\n# Process commas :\ntext_encrypted=list(map(lambda x: re.sub(\"'\", '', x), text_encrypted))\ntext_encrypted=list(map(lambda x: re.sub(\",\", ' COMMA', x), text_encrypted))\n\ntext=list(map(lambda x: re.sub(\"'\", '', x), text))\ntext=list(map(lambda x: re.sub(\",\", ' COMMA', x),text))\n\n# Getting rid of punctuation\nexclude = set(string.punctuation)\ntext_encrypted=list(map(lambda x: ''.join(ch for ch in x if ch not in exclude), text_encrypted))\ntext=list(map(lambda x: ''.join(ch for ch in x if ch not in exclude), text))\n\n# Getting rid of digits\nremove_digits = str.maketrans('', '', digits)\ntext_encrypted=list(map(lambda x: x.translate(remove_digits), text_encrypted))\ntext=list(map(lambda x: x.translate(remove_digits), text))\n\nprint(\"DONE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sample Processing\n\n# Appending SOS andEOS to target data : \ntext = list(map(lambda x : 'SOS_ '+ x + ' _EOS', text))\n\n# Create word dictionaries :\nen_words=set()\nfor line in text_encrypted:\n    for word in line.split():\n        if word not in en_words:\n            en_words.add(word)\n    \nde_words=set()\nfor line in text:\n    for word in line.split():\n        if word not in de_words:\n            de_words.add(word)\n            \n# get lengths and sizes :\nnum_en_words = len(en_words)\nnum_de_words = len(de_words)\n\nmax_en_words_per_sample = max([len(sample.split()) for sample in text_encrypted])+5\nmax_de_words_per_sample = max([len(sample.split()) for sample in text])+5\n\nnum_en_samples = len(text_encrypted)\nnum_de_samples = len(text)\n\n# Get lists of words :\ninput_words = sorted(list(en_words))\ntarget_words = sorted(list(de_words))\n\nen_token_to_int = dict()\nen_int_to_token = dict()\n\nde_token_to_int = dict()\nde_int_to_token = dict()\n\n#Tokenizing the words ( Convert them to numbers ) :\nfor i,token in enumerate(input_words):\n    en_token_to_int[token] = i\n    en_int_to_token[i]     = token\n\nfor i,token in enumerate(target_words):\n    de_token_to_int[token] = i\n    de_int_to_token[i]     = token\n\n# initiate numpy arrays to hold the data that our seq2seq model will use:\nencoder_input_data = np.zeros(\n    (num_en_samples, max_en_words_per_sample),\n    dtype='float32')\ndecoder_input_data = np.zeros(\n    (num_de_samples, max_de_words_per_sample),\n    dtype='float32')\ndecoder_target_data = np.zeros(\n    (num_de_samples, max_de_words_per_sample, num_de_words),\n    dtype='float32')\n\n# Process samples, to get input, output, target data:\nfor i, (input_text, target_text) in enumerate(zip(text_encrypted, text)):\n    for t, word in enumerate(input_text.split()):\n        encoder_input_data[i, t] = en_token_to_int[word]\n    for t, word in enumerate(target_text.split()):\n        # decoder_target_data is ahead of decoder_input_data by one timestep\n        decoder_input_data[i, t] = de_token_to_int[word]\n        if t > 0:\n            # decoder_target_data will be ahead by one timestep\n            # and will not include the start character.\n            decoder_target_data[i, t - 1, de_token_to_int[word]] = 1.\n\nprint(\"DONE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing needed dependencies :\nimport pandas as pd\nimport numpy as np\nimport string\nfrom string import digits\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, LSTM, Input, Embedding, TimeDistributed, Flatten, Dropout\nfrom keras.callbacks import ModelCheckpoint\n\nfrom keras.layers import CuDNNLSTM\n\nprint(\"DONE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining some constants: \nvec_len       = 300   # Length of the vector that we willl get from the embedding layer\nlatent_dim    = 1024  # Hidden layers dimension \ndropout_rate  = 0.2   # Rate of the dropout layers\nbatch_size    = 64    # Batch size\nepochs        = 10    # Number of epochs\n\nprint(\"DONE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define an input sequence and process it.\n# Input layer of the encoder :\nencoder_input = Input(shape=(None,))\n\n# Hidden layers of the encoder :\nencoder_embedding = Embedding(input_dim = num_en_words, output_dim = vec_len)(encoder_input)\nencoder_dropout   = (TimeDistributed(Dropout(rate = dropout_rate)))(encoder_embedding)\nencoder_LSTM      = LSTM(latent_dim, return_sequences=True)(encoder_dropout)\n#encoder_LSTM      = CuDNNLSTM(latent_dim, return_sequences=True)(encoder_dropout)\n\n\n# Output layer of the encoder :\n#encoder_LSTM2_layer = CuDNNLSTM(latent_dim, return_state=True)\nencoder_LSTM2_layer = LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder_LSTM2_layer(encoder_LSTM)\n\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]\n\nprint(\"DONE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the decoder, using `encoder_states` as initial state.\n# Input layer of the decoder :\ndecoder_input = Input(shape=(None,))\n\n# Hidden layers of the decoder :\ndecoder_embedding_layer = Embedding(input_dim = num_de_words, output_dim = vec_len)\ndecoder_embedding = decoder_embedding_layer(decoder_input)\n\ndecoder_dropout_layer = (TimeDistributed(Dropout(rate = dropout_rate)))\ndecoder_dropout = decoder_dropout_layer(decoder_embedding)\n\n#decoder_LSTM_layer = CuDNNLSTM(latent_dim, return_sequences=True)\ndecoder_LSTM_layer = LSTM(latent_dim, return_sequences=True)\ndecoder_LSTM = decoder_LSTM_layer(decoder_dropout, initial_state = encoder_states)\n\n#decoder_LSTM_2_layer = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_LSTM_2_layer = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_LSTM_2,_,_ = decoder_LSTM_2_layer(decoder_LSTM)\n\n# Output layer of the decoder :\ndecoder_dense = Dense(num_de_words, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_LSTM_2)\n\nprint(\"DONE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model([encoder_input, decoder_input], decoder_outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install --upgrade livelossplot\n#from livelossplot import PlotLossesKeras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from keras.callbacks import EarlyStopping\n#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    shuffle=True,\n                    validation_split=0.2,\n                    callbacks=callbacks_list,\n                    verbose=1\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.save('model.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}